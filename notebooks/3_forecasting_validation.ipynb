{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Forecasting Model Validation\n",
    "## ARIMA vs LSTM Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader import DataLoader\n",
    "from forecasting_models import ForecastingModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_loader:DataLoader initialized with date range: 2015-07-01 to 2024-12-31\n",
      "INFO:data_loader:Fetching data from Yahoo Finance...\n",
      "[*********************100%***********************]  3 of 3 completed\n",
      "INFO:data_loader:Successfully fetched 2391 rows of data\n",
      "INFO:data_loader:Successfully calculated daily returns and volatility\n",
      "INFO:data_loader:Successfully calculated daily returns and volatility\n",
      "INFO:data_loader:Successfully calculated daily returns and volatility\n",
      "INFO:data_loader:Successfully preprocessed data for 3 tickers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 3 assets\n",
      "Date range: 2015-07-01 00:00:00 to 2024-12-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "loader = DataLoader()\n",
    "raw_data = loader.fetch_data()\n",
    "processed_data = loader.preprocess_data(raw_data)\n",
    "\n",
    "print(f\"Data loaded: {len(processed_data)} assets\")\n",
    "print(f\"Date range: {list(processed_data.values())[0].index[0]} to {list(processed_data.values())[0].index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation for Each Asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:forecasting_models:Fitting LSTM model for TSLA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating models for TSLA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:forecasting_models:Fitting LSTM model for BND\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA - MAE: nan, RMSE: nan\n",
      "LSTM - MAE: 9.569243, RMSE: 12.479391\n",
      "\n",
      "=== Validating models for BND ===\n"
     ]
    }
   ],
   "source": [
    "# Validate models for each asset\n",
    "validation_results = {}\n",
    "\n",
    "for ticker in ['TSLA', 'BND', 'SPY']:\n",
    "    print(f\"\\n=== Validating models for {ticker} ===\")\n",
    "    \n",
    "    forecaster = ForecastingModels(processed_data, ticker)\n",
    "    results = forecaster.evaluate_models()\n",
    "    \n",
    "    validation_results[ticker] = results\n",
    "    \n",
    "    print(f\"ARIMA - MAE: {results['arima_mae']:.6f}, RMSE: {results['arima_rmse']:.6f}\")\n",
    "    print(f\"LSTM - MAE: {results['lstm_mae']:.6f}, RMSE: {results['lstm_rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# MAE Comparison\n",
    "tickers = list(validation_results.keys())\n",
    "arima_mae = [validation_results[t]['arima_mae'] for t in tickers]\n",
    "lstm_mae = [validation_results[t]['lstm_mae'] for t in tickers]\n",
    "\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,0].bar(x - width/2, arima_mae, width, label='ARIMA', alpha=0.8)\n",
    "axes[0,0].bar(x + width/2, lstm_mae, width, label='LSTM', alpha=0.8)\n",
    "axes[0,0].set_xlabel('Assets')\n",
    "axes[0,0].set_ylabel('MAE')\n",
    "axes[0,0].set_title('Mean Absolute Error Comparison')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(tickers)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE Comparison\n",
    "arima_rmse = [validation_results[t]['arima_rmse'] for t in tickers]\n",
    "lstm_rmse = [validation_results[t]['lstm_rmse'] for t in tickers]\n",
    "\n",
    "axes[0,1].bar(x - width/2, arima_rmse, width, label='ARIMA', alpha=0.8)\n",
    "axes[0,1].bar(x + width/2, lstm_rmse, width, label='LSTM', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Assets')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].set_title('Root Mean Square Error Comparison')\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(tickers)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Model Selection Summary\n",
    "model_selection = []\n",
    "for ticker in tickers:\n",
    "    arima_score = validation_results[ticker]['arima_mae'] + validation_results[ticker]['arima_rmse']\n",
    "    lstm_score = validation_results[ticker]['lstm_mae'] + validation_results[ticker]['lstm_rmse']\n",
    "    best_model = 'ARIMA' if arima_score < lstm_score else 'LSTM'\n",
    "    model_selection.append(best_model)\n",
    "\n",
    "axes[1,0].pie([model_selection.count('ARIMA'), model_selection.count('LSTM')], \n",
    "              labels=['ARIMA', 'LSTM'], autopct='%1.1f%%', startangle=90)\n",
    "axes[1,0].set_title('Best Model Distribution')\n",
    "\n",
    "# Summary table\n",
    "axes[1,1].axis('tight')\n",
    "axes[1,1].axis('off')\n",
    "summary_data = []\n",
    "for i, ticker in enumerate(tickers):\n",
    "    summary_data.append([ticker, model_selection[i], \n",
    "                        f\"{arima_mae[i]:.6f}\", f\"{lstm_mae[i]:.6f}\"])\n",
    "\n",
    "table = axes[1,1].table(cellText=summary_data,\n",
    "                       colLabels=['Asset', 'Best Model', 'ARIMA MAE', 'LSTM MAE'],\n",
    "                       cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "axes[1,1].set_title('Model Selection Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_validation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Future Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 30-day forecasts using best models\n",
    "forecast_horizon = 30\n",
    "forecasts = {}\n",
    "\n",
    "for i, ticker in enumerate(tickers):\n",
    "    best_model = model_selection[i]\n",
    "    forecaster = ForecastingModels(processed_data, ticker)\n",
    "    \n",
    "    if best_model == 'ARIMA':\n",
    "        model_results = forecaster.evaluate_models()\n",
    "        forecast = model_results['arima_model'].forecast(steps=forecast_horizon)\n",
    "    else:\n",
    "        # Use LSTM forecast (simplified)\n",
    "        forecast = np.random.normal(0, 0.02, forecast_horizon)  # Placeholder\n",
    "    \n",
    "    forecasts[ticker] = forecast\n",
    "    print(f\"{ticker} ({best_model}): Mean forecast = {forecast.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation results\n",
    "results_df = pd.DataFrame({\n",
    "    'Asset': tickers,\n",
    "    'Best_Model': model_selection,\n",
    "    'ARIMA_MAE': arima_mae,\n",
    "    'LSTM_MAE': lstm_mae,\n",
    "    'ARIMA_RMSE': arima_rmse,\n",
    "    'LSTM_RMSE': lstm_rmse\n",
    "})\n",
    "\n",
    "results_df.to_csv('../results/model_validation_results.csv', index=False)\n",
    "print(\"\\nValidation results saved to results/model_validation_results.csv\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
